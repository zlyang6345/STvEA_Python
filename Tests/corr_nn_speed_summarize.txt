1.
           cor_dist_df = query.apply(
           lambda row: data.apply(lambda inner_row: 1 - np.corrcoef(row, inner_row)[0, 1], axis=1),
           axis=1)

           4 run's average = 708 sec for 3000 cells.

2.
            cor_dist_df = query.apply(
            lambda row: data.apply(lambda inner_row: 1 - row.corr(inner_row, method="pearson"), axis=1),
            axis=1)

            round 1 = 56.46 sec
            round 2 = 70 sec

3.
            query_dask = dd.from_pandas(query, npartitions=4)
            data_dask = dd.from_pandas(data, npartitions=4)
            cor_dist_df = query_dask.map_partitions(
                lambda df: df.apply(
                    lambda row: data_dask.map_partitions(
                        lambda df_inner: df_inner.apply(
                            lambda inner_row: 1 - row.corr(inner_row, method="pearson"), axis=1)
                    ).compute(scheduler='processes'), axis=1)
            ).compute(scheduler='processes')

            round 1 = too slow, does not terminate

4.
                        # multithread
            import concurrent.futures
            chunk = ceil(query.shape[0] / npartition)

            queries = list()
            for i in range(npartition):
                queries.append(query.iloc[chunk * i: chunk * (i+1), :])

            def helper(query_sub):

                cor_dist_df_sub = query_sub.apply(
                    lambda row: data.apply(lambda inner_row: 1 - np.corrcoef(row, inner_row)[0, 1], axis=1),
                    axis=1)

                return cor_dist_df_sub

            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = [executor.submit(helper, queries[i]) for i in range(npartition)]
                cor_dist_df_sub_list = [future.result() for future in futures]

            cor_dist_df = pd.concat(cor_dist_df_sub_list)

            4 runs' average = 873 for 3000 cells.

5.
            import concurrent.futures
            chunk_num = ceil(query.shape[0] / npartition)
            queries = [query.iloc[chunk_num * i: chunk_num * (i + 1), :] for i in range(npartition)]
            with concurrent.futures.ProcessPoolExecutor() as executor:
                futures = [executor.submit(Mapping.helper, queries[i], data) for i in range(npartition)]
                cor_dist_df_sub_list = [future.result() for future in futures]

            cor_dist_df = pd.concat(cor_dist_df_sub_list)

            4 runs' average = 449 sec for 3000 cells, 3 partitions.
            4 runs' average = 439 sec for 3000 cells, 6 partitions.

6.
            # invoke R's function
            # the fastest option
            from rpy2.robjects import pandas2ri
            import rpy2.robjects as ro
            from rpy2.robjects.conversion import localconverter
            with localconverter(ro.default_converter + pandas2ri.converter):

                # get the conversion context
                conv = ro.conversion.get_conversion()
                # convert pandas DataFrames to R data frames
                r_data = conv.py2rpy(data)
                r_query = conv.py2rpy(query)
                # define the R function as a string
                func = """
                function(data, query){
                    cor_dist_df <- apply(query, 1, function(row) {
                                        apply(data, 1, function(inner_row) {1 - cor(row, inner_row)})
                                    })
                    # convert to a data frame
                    cor_dist_df <- as.data.frame(cor_dist_df)
                    return(cor_dist_df)
                }
                """
                # execute the R function with ro.r()
                cor_nn_r_internal = ro.r(func)
                # call the R function with the data
                result = cor_nn_r_internal(r_data, r_query)
                # convert R DataFrame to pandas DataFrame
                cor_dist_df = conv.rpy2py(result)

                4 runs' average time: 88 sec for 3000 cells.

7.
            from rpy2.robjects import pandas2ri
            import rpy2.robjects as ro
            from rpy2.robjects.conversion import localconverter
            from rpy2.robjects.packages import importr

            base = importr('base')

            with localconverter(ro.default_converter + pandas2ri.converter):
                # get the conversion context
                conv = ro.conversion.get_conversion()

                # convert pandas DataFrames to R data frames
                r_data = conv.py2rpy(data)
                r_query = conv.py2rpy(query)

                # define the R function as a string
                func = """
                function(
                  data,
                  query = data,
                  k = 5
                ) {
                      t_data <- t(data)
                      query <- as.matrix(query)
                      neighbors <- matrix(rep(0, k*nrow(query)), ncol=k)
                      distances <- matrix(rep(0, k*nrow(query)), ncol=k)

                      for (i in 1:nrow(query)) {
                          cor_dist <- 1 - cor(query[i,], t_data)
                          idx <- order(cor_dist)[1:k]
                          neighbors[i,] <- idx
                          distances[i,] <- cor_dist[idx]
                      }
                      neighbors = as.data.frame(apply(neighbors, 2, as.integer) - 1)
                      distances = as.data.frame(distances)
                      return(list(nn_idx=neighbors, nn_dists=distances))
                }
                """

                # execute the R function with ro.r()
                cor_nn_r_internal = ro.r(func)

                # call the R function with the data
                result = cor_nn_r_internal(r_data, r_query, k)

                # Convert the R list of data frames to a Python dictionary of pandas data frames
                py_result = {key: conv.rpy2py(result[key]) for key in result.keys()}

                return py_result
                4 runs' average time: 2 sec for 3000 cells ???
